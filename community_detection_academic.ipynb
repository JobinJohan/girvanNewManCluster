{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Community detection in an academic network\n",
    "## Aim\n",
    "<ol>\n",
    "  <li> Load social graph</li>\n",
    "  <li> Run community detection and centrality methods</li>\n",
    "  <li> Visualize the network </li>\n",
    "</ol>\n",
    "\n",
    "## Tasks\n",
    "<ol>\n",
    "    <li> <strong>Load the dataset:</strong> Load the Author Network dataset provided at <a href:\"https://aminer.org/lab-datasets/soinf/\">https://aminer.org/lab-datasets/soinf/</a>\n",
    "The graph consists of authors and coauthor relationships.\n",
    "  </li>\n",
    "    <li> <strong>Implementation:</strong> \n",
    "        <ol>\n",
    "            <li> Implement Girvan-Newman clustering algorithm till 10th iteration level.</li>\n",
    "            <li> Implement Pagerank algorithm.</li>\n",
    "            <li> Implement Betweenness centrality measure</li>\n",
    "        <strong>Use the previous implementation, perform the following tasks</strong>\n",
    "            <li> Use Girvan-Newman algorithm to find clusters of authors</li>\n",
    "            <li> Find the top-10 authors with highest betweenness centrality</li>      \n",
    "        </ol>\n",
    "    </li>\n",
    "    <li> <strong>Visualization: </strong> \n",
    "        <ol>\n",
    "            <li> Visualize the output of Girvan-Newman algorithm by coloring nodes according to their assigned groups</li>\n",
    "            <li> Visualize the network and highlight the top 10 authors with the highest betweeness centrality and top 10 edges with the highest betweenness centrality</li>      \n",
    "        </ol>\n",
    "    </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import networkx as nx\n",
    "from pprint import pprint\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Utility class to pretty print dictionary\n",
    "class DictTable(dict):\n",
    "    # Overridden dict class which takes a dict in the form {'a': 2, 'b': 3},\n",
    "    # and renders an HTML Table in IPython Notebook.\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table width=100%>\"]\n",
    "        for key, value in iter(self.items()):\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td>{0}</td>\".format(key))\n",
    "            html.append(\"<td>{0}</td>\".format(str(len(value))+\" \"+\"graph[s] of size: \"+str([len(x) for x in value])+\" nodes\"))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "print(\"> All necessary modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "DATASET = \"./data\"\n",
    "\n",
    "def loadData(directoryPath):\n",
    "    \"\"\"Load the data of all files of the folder given in parameter.\"\"\"\n",
    "    files = os.listdir(directoryPath)\n",
    "    # dictionary wrt to the following format: {topic1:[[network1],[network2]]}\n",
    "    allGraphsOfEachTopic = {}\n",
    "    for file in files:\n",
    "        topic = \"\"\n",
    "        if \"T107\" in file:\n",
    "            topic = \"Web Services\"\n",
    "        elif \"T131\" in file:\n",
    "            topic = \"Bayesian Networks/Belief function\"\n",
    "        elif \"T144\" in file:\n",
    "            topic = \"Web Mining/Information Fusion\"\n",
    "        elif \"T145\" in file:\n",
    "            topic = \"Semantic Web/Description Logics\"\n",
    "        elif \"T162\" in file:\n",
    "            topic = \"Machine Learning\"\n",
    "        elif \"T16\" in file:\n",
    "            topic = \"Data Mining/Association Rules\"\n",
    "        elif \"T24\" in file:\n",
    "            topic = \"Database Systems/XML Data\"\n",
    "        elif \"T75\" in file:\n",
    "            topic = \"Information Retrieval\"\n",
    "        else:\n",
    "            topic = \"Unknown\"\n",
    "\n",
    "        graphToBuild = nx.Graph()\n",
    "        \n",
    "        #Â for naming purposes\n",
    "        subStart = file.find('sub')\n",
    "        subEnd = file.find('.')\n",
    "        graphToBuild.name = topic.replace('/', ' ').replace(' ', '') + '_{}'.format(file[subStart:subEnd])\n",
    "\n",
    "        # constant\n",
    "        VERTEX = 0\n",
    "        EDGE = 1\n",
    "        TRIANGLE = 2\n",
    "\n",
    "        f = open(DATASET+\"/\"+file)\n",
    "\n",
    "        # Vertices: Int \"String\" Int -> NodeID, personName, #papers\n",
    "        # Edges: Int Int Int -> sourceNodeID, DestNodeID, #coauthoredPapers\n",
    "        # Triangles: Int,Int,Int,Int -> NodeID1, NodeID2, NodeID3, #coauthoredPapers\n",
    "        for line in f:\n",
    "            if \"*Vertices\" in line:\n",
    "                typeOfLine = VERTEX\n",
    "            elif \"*Edges\" in line:\n",
    "                typeOfLine = EDGE\n",
    "            elif \"*Triangles\" in line:\n",
    "                typeOfLine = TRIANGLE\n",
    "            else:\n",
    "                if typeOfLine == VERTEX:\n",
    "                    graph_edge_list = []\n",
    "                    \n",
    "                    for index, s in enumerate(re.split('\"', line)):\n",
    "                        if (index == 1):\n",
    "                            graph_edge_list.append(s)\n",
    "                        else:\n",
    "                            graph_edge_list.append(s.replace(' ','').replace('\\n', ''))\n",
    "                            \n",
    "                    graphToBuild.add_node(graph_edge_list[0], name=graph_edge_list[1], nbpapers=graph_edge_list[2])\n",
    "                elif typeOfLine == EDGE:\n",
    "                    graph_edge_list = line.split()\n",
    "                    graphToBuild.add_edge(graph_edge_list[0], graph_edge_list[1], coauthoredPapers=graph_edge_list[2])\n",
    "                elif typeOfLine == TRIANGLE:\n",
    "                    graph_edge_list = line.split(',')\n",
    "                    graph_edge_list[3] = graph_edge_list[3].replace('\\n', '')\n",
    "                    graphToBuild.add_edge(graph_edge_list[0], graph_edge_list[1], coauthoredPapersTriangle=graph_edge_list[3])\n",
    "                    graphToBuild.add_edge(graph_edge_list[0], graph_edge_list[2], coauthoredPapersTriangle=graph_edge_list[3])\n",
    "                    graphToBuild.add_edge(graph_edge_list[1], graph_edge_list[2], coauthoredPapersTriangle=graph_edge_list[3])\n",
    "\n",
    "        if topic in allGraphsOfEachTopic:\n",
    "            allGraphsOfEachTopic[topic].append(graphToBuild)\n",
    "        else:\n",
    "            allGraphsOfEachTopic[topic] = [graphToBuild]\n",
    "\n",
    "    return allGraphsOfEachTopic\n",
    "\n",
    "allData = loadData(DATASET)\n",
    "print(\"> All data loaded\")\n",
    "DictTable(allData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "## 2A. Implementation of Girvan-Newman clustering till 10th iteration level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def listAuthorsClusters(G):\n",
    "    if not os.path.exists('./Output/GirvanNewman/ClustersTxt'):\n",
    "        os.makedirs('./Output/GirvanNewman/ClustersTxt')\n",
    "    \n",
    "    connected_components = nx.connected_component_subgraphs(G)\n",
    "    \n",
    "    f = open('./Output/GirvanNewman/ClustersTxt/{}.txt'.format(G.name), 'w')\n",
    "    for index, sg in enumerate(connected_components):\n",
    "        f.write('######## Cluster {:2} #######\\n'.format(index))\n",
    "        for node in sg.nodes:\n",
    "            f.write('> {}\\n'.format(G.node[node]['name']))\n",
    "        f.write('\\n\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def girvanNewmanClustering(graph, nbIteration, visualization = False):\n",
    "    print('Girvan-Newman on graph \"{}\", {} iterations'.format(graph.name, nbIteration))\n",
    "    \n",
    "    if visualization == True:\n",
    "         # Draw first plot in figure and use variable cnt to handle positions\n",
    "        fig = plt.figure(figsize=(50, 50))\n",
    "        drawGraphs(graph, 1)\n",
    "        cnt = 2\n",
    "   \n",
    "    \n",
    "    # If there are no edge in the graph\n",
    "    if len(list(graph.edges)) == 0:\n",
    "        return \"Empty graph\"\n",
    "\n",
    "    # While there are edges in the graph and that the number of iteration is biggger than 0\n",
    "    while(len(list(graph.edges)) > 0 and nbIteration > 0):\n",
    "        print(\"Remaining iterations: \", nbIteration)\n",
    "        nbIteration = nbIteration - 1\n",
    "        \n",
    "        # Compute all shortest path between all nodes,transform this into a numpy array, put 0 at each element\n",
    "        # under the diagonal of the matrix\n",
    "        #allShortestPath = np.triu(np.asarray(list(nx.all_pairs_shortest_path(graph))),-2)\n",
    "        \n",
    "        # Delete all rows containging only 0\n",
    "        #allShortestPath = allShortestPath[~np.all(allShortestPath == 0, axis=1)]\n",
    "        print(\"test111\")\n",
    "        allShortestPaths = []\n",
    "        for index, vi in enumerate(np.asarray(list(nx.all_pairs_shortest_path(graph)))):\n",
    "            test = list(vi[1].values())\n",
    "            test2= sorted(test, key=lambda x: int(x[-1]))[index:]\n",
    "            for path in test2:\n",
    "                allShortestPaths.append(path)\n",
    "        print(\"test222\", len(allShortestPaths))\n",
    "        counter=0\n",
    "        \n",
    "        if len(allShortestPaths) ==0:\n",
    "            break\n",
    "        \n",
    "        # Get all edges of the current graph\n",
    "        edges = np.asarray(list(graph.edges))\n",
    "        \n",
    "        # Variable to store the highest edge and the centrality associated with it\n",
    "        highestEdge = \"\"\n",
    "        highestScore = -float('inf')\n",
    "        \n",
    "        for edge in edges:\n",
    "            counter+=1\n",
    "            print(counter)\n",
    "            centrality = edgesBetweenessCentrality(allShortestPaths,edge)\n",
    "            # Keep the edge with the highest centrality\n",
    "            if centrality > highestScore:\n",
    "                highestScore = centrality\n",
    "                highestEdge = edge\n",
    "                \n",
    "        # Remove the edge with the highest centrality\n",
    "        graph.remove_edge(highestEdge[0], highestEdge[1])\n",
    "        \n",
    "        if visualization == True:\n",
    "            # Draw the graph with the removed edge into the figure\n",
    "            drawGraphs(graph, cnt)\n",
    "            cnt += 1\n",
    "        \n",
    "    \n",
    "    if visualization == True:\n",
    "        # Draw the original graph with colors to detect communities and save the figure\n",
    "        print(\"Drawing...\")\n",
    "        drawColoredGraph(graph)\n",
    "        print(\"Drawing done\")\n",
    "        if not os.path.exists('./Output/GirvanNewman/Figures'):\n",
    "            os.makedirs('./Output/GirvanNewman/Figures')\n",
    "    \n",
    "        plt.savefig(\"./Output/GirvanNewman/Figures/{}.png\".format(graph.name))\n",
    "        print(\"> Figure saved\")\n",
    "    \n",
    "    print('Saving clusters composition...')\n",
    "    listAuthorsClusters(graph)\n",
    "    print('> Clusters composition saved')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B. Implementation of Pagerank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRankCentrality(graph, alpha, beta):\n",
    "    # Transposition of matrix\n",
    "    adjacencyMatrix = nx.to_numpy_matrix(graph, weight='None')\n",
    "    amTransposed = np.transpose(adjacencyMatrix)\n",
    "\n",
    "    # Diagonal Matrix\n",
    "    diagonalMatrix = np.zeros([adjacencyMatrix.shape[0], adjacencyMatrix.shape[1]])\n",
    "    row, col = np.diag_indices(diagonalMatrix.shape[0])\n",
    "    # Compute the values that have to be filled into the diagonal\n",
    "    diagonalMatrix[row, col] = [1 / degree[1] for degree in list(graph.degree())]\n",
    "\n",
    "    # Identity matrix\n",
    "    identityMatrix = np.identity(adjacencyMatrix.shape[0])\n",
    "\n",
    "    # Vector of ones\n",
    "    ones = np.ones((adjacencyMatrix.shape[0], 1))\n",
    "    pageRankCentrality = np.dot(beta * np.linalg.inv((identityMatrix - np.dot(alpha * amTransposed, diagonalMatrix))), ones)\n",
    "\n",
    "    return pageRankCentrality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2C. Implementation of the betweenness centrality measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgesBetweenessCentrality(allShortestPath, edge):\n",
    "    \"\"\"computes the betweenness centrality of a given edge\"\"\"\n",
    "    # Initialization of variables to compute centrality of each edge\n",
    "    centrality=0\n",
    "    nbPathsIncludingEdge=0\n",
    "    counterShortestPath=0\n",
    "            \n",
    "    # For all shortestPath, count the number of them that contain the current edge \n",
    "    for vi in allShortestPath:\n",
    "        nbPathsIncludingEdge += edgeIsInPath([edge[0], edge[1]], vi)\n",
    "        counterShortestPath+=1\n",
    "            \n",
    "    # Centrality = Number of shortest paths including the edge / total number of shortest paths       \n",
    "    centrality += nbPathsIncludingEdge / counterShortestPath\n",
    "    return centrality\n",
    "\n",
    "def edgeIsInPath(edge, path):\n",
    "    \"\"\"checks if an edge is contained in a path of an undirected graph (the path is a list of nodes)\"\"\"\n",
    "    edge1= [edge[0],edge[1]]\n",
    "    edge2= [edge[1],edge[0]]\n",
    "    if all(i in path for i in edge1) or all(i in path for i in edge2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def nodeIsInPath(node, path):\n",
    "    \"\"\"checks if an edge is contained in a path of an undirected graph (the path is a list of nodes)\"\"\"\n",
    "    if node in path:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def nodesBetweennessCentrality(listOfAllShortestPath, node, graph):\n",
    "    \"\"\"betweenness centrality of a given node: used in task 2E\"\"\"\n",
    "    centrality = 0\n",
    "    nbPathsIncludingNode=0\n",
    "    counterShortestPath=0\n",
    "    \n",
    "    for vi in listOfAllShortestPath:\n",
    "        nbPathsIncludingNode += nodeIsInPath(node, vi)\n",
    "        counterShortestPath+=1\n",
    "        \n",
    "    centrality = nbPathsIncludingNode/counterShortestPath\n",
    "    return centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D. Use Girvan-Newman algorithm to find clusters of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs Girvan-Newman for every graph in the dataset\n",
    "for topic, listOfGraphs in allData.items():\n",
    "    print('>>> Topic \"{}\"\"'.format(topic))\n",
    "    for graph in listOfGraphs:\n",
    "        print('> Girvan-Newman clustering for {}'.format(graph.name))\n",
    "        girvanNewmanClustering(graph.copy(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2E. Find the top-10 authors with highest betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the top-10 authors for each graph of the dataset        \n",
    "for topic, listOfGraphs in allData.items():\n",
    "    print('Topic \"{}\"\"'.format(topic))\n",
    "    for graph in listOfGraphs:\n",
    "        results = []\n",
    "        allShortestPaths = []\n",
    "        for index, vi in enumerate(np.asarray(list(nx.all_pairs_shortest_path(graph)))):\n",
    "            test = list(vi[1].values())\n",
    "            test2= sorted(test, key=lambda x: int(x[-1]))[index+1:]\n",
    "            for path in test2:\n",
    "                allShortestPaths.append(path)\n",
    "        for node in graph.nodes:\n",
    "            results.append( (nodesBetweennessCentrality(allShortestPaths, node, graph), graph.node[node]['name']) )\n",
    "        print('Top-10 authors for {}'.format(graph.name))\n",
    "        for tuple in sorted(results, reverse=True)[:10]:\n",
    "            print('>>> {}'.format(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example with a single graph: TO DELETE FOR THE FINALE VERSION BUT IS CURRENTLY A PROOF THAT THE ALGORITHM WORKS \n",
    "                            #AS IS PRODUCES THE SAME RESULT THAN THE NETWORKX FUNCTION \n",
    "graph = allData[\"Web Mining/Information Fusion\"][2].copy()\n",
    "print(\"Here are the results given by networkX \\n\",nx.betweenness_centrality(graph,endpoints=True))\n",
    "results = []\n",
    "\n",
    "allShortestPaths = []\n",
    "for index, vi in enumerate(np.asarray(list(nx.all_pairs_shortest_path(graph)))):\n",
    "    test = list(vi[1].values())\n",
    "    test2= sorted(test, key=lambda x: int(x[-1]))[index+1:]\n",
    "    for path in test2:\n",
    "        allShortestPaths.append(path)\n",
    "        \n",
    "for node in graph.nodes:\n",
    "    results.append( (nodesBetweennessCentrality(allShortestPaths, node, graph), graph.node[node]['name']) )\n",
    "print('Top-10 authors for {}'.format(graph.name))\n",
    "for tuple in sorted(results, reverse=True)[:10]:\n",
    "    print('>>> {}'.format(tuple))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A. Visualize the output of Girvan-Newman algorithm by coloring nodes according to their assigned groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(G):\n",
    "    \"\"\"draws the given graph and displays the labels of each edge\"\"\"\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True)\n",
    "    nx.draw_networkx_edge_labels(G, pos)\n",
    "    plt.show()\n",
    "\n",
    "def drawGraphs(G, cnt):\n",
    "    \"\"\"Draw all different subgraphs on the same picture\"\"\"\n",
    "    test = plt.subplot(4, 3, cnt)\n",
    "    if (cnt == 1):\n",
    "        test.title.set_text('Initial state of the graph')\n",
    "    else:\n",
    "        test.title.set_text('State of the graph at iteration: {}, number of communities: {}'.format(cnt-1, nx.number_connected_components(G)))\n",
    "    test.set_yticklabels([])\n",
    "    test.set_xticklabels([])\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx(G, pos)\n",
    "    nx.draw_networkx_edge_labels(G, pos)\n",
    "\n",
    "def drawColoredGraph(G):\n",
    "    \"\"\"# draws a graph where communities are drawn using different colors\"\"\"\n",
    "    test = plt.subplot(4,3,12)\n",
    "    test.set_yticklabels([])\n",
    "    test.set_xticklabels([])\n",
    "    test.title.set_text('Resulting communities ({})'.format(nx.number_connected_components(G)))\n",
    "    pos = nx.spring_layout(G)\n",
    "    connected_components = nx.connected_component_subgraphs(G)\n",
    "    for index, sg in enumerate(connected_components):\n",
    "        r = lambda: random.randint(0,255)\n",
    "        randomColor =('#%02X%02X%02X' % (r(),r(),r()))\n",
    "        nx.draw_networkx(sg, pos = pos, node_color = str(randomColor), with_labels= False, edgelist=[])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs Girvan-Newman for every graph in the dataset and visualize the output\n",
    "for topic, listOfGraphs in allData.items():\n",
    "    print('>>> Topic \"{}\"\"'.format(topic))\n",
    "    for graph in listOfGraphs:\n",
    "        print('> Girvan-Newman clustering for {}'.format(graph.name))\n",
    "        girvanNewmanClustering(graph.copy(), 10, visualization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. Visualize the network and highlight the top 10 authors with the highest betweenness centrality and top 10 edges with the highest betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
