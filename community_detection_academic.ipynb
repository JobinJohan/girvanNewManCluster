{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Community detection in an academic network\n",
    "## Aim\n",
    "<ol>\n",
    "  <li> Load social graph</li>\n",
    "  <li> Run community detection and centrality methods</li>\n",
    "  <li> Visualize the network </li>\n",
    "</ol>\n",
    "\n",
    "## Tasks\n",
    "<ol>\n",
    "    <li> <strong>Load the dataset:</strong> Load the Author Network dataset provided in <a href:\"https://aminer.org/lab-datasets/soinf/\">https://aminer.org/lab-datasets/soinf/</a>\n",
    "The graph consists of authors and coauthor relationships.\n",
    "  </li>\n",
    "    <li> <strong>Implementation:</strong> \n",
    "        <ol>\n",
    "            <li> Implement Girvan-Newman clustering algorithm till 10th iteration level.</li>\n",
    "            <li> Implement Pagerank algorithm.</li>\n",
    "            <li> Implement Betweenness centrality measure</li>        \n",
    "        </ol>\n",
    "        Use the previous implementation, perform the following tasks\n",
    "        <ol>\n",
    "            <li> Use Girvan-Newman algorithm to find clusters of authors</li>\n",
    "            <li> Find the top-10 authors with highest betweenness centrality</li>      \n",
    "        </ol>\n",
    "    </li>\n",
    "    <li> <strong>Visualization: </strong> \n",
    "        <ol>\n",
    "            <li> Visualize the output of Girvan-Newman algorithm by coloring nodes ccording to their assigned groups</li>\n",
    "            <li> Visualize the network and highlight the top 10 authors with the highest betweeness centrality and top 10 edges with the highest betweenness centrality</li>      \n",
    "        </ol>\n",
    "    </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> All necessary modules imported\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "import networkx as nx\n",
    "from pprint import pprint\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Utility class to pretty print dictionary\n",
    "class DictTable(dict):\n",
    "    # Overridden dict class which takes a dict in the form {'a': 2, 'b': 3},\n",
    "    # and renders an HTML Table in IPython Notebook.\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table width=100%>\"]\n",
    "        for key, value in iter(self.items()):\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td>{0}</td>\".format(key))\n",
    "            html.append(\"<td>{0}</td>\".format(str(len(value))+\" \"+\"graph[s]\"))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "print(\"> All necessary modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> All data loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width=100%><tr><td>Web Services</td><td>2 graph[s]</td></tr><tr><td>Bayesian Networks/Belief function</td><td>1 graph[s]</td></tr><tr><td>Web Mining/Information Fusion</td><td>3 graph[s]</td></tr><tr><td>Semantic Web/Description Logics</td><td>2 graph[s]</td></tr><tr><td>Data Mining/Association Rules</td><td>2 graph[s]</td></tr><tr><td>Database Systems/XML Data</td><td>1 graph[s]</td></tr><tr><td>Information Retrieval</td><td>2 graph[s]</td></tr></table>"
      ],
      "text/plain": [
       "{'Web Services': [<networkx.classes.graph.Graph at 0x1d2427bf780>,\n",
       "  <networkx.classes.graph.Graph at 0x1d2427ea2b0>],\n",
       " 'Bayesian Networks/Belief function': [<networkx.classes.graph.Graph at 0x1d24536def0>],\n",
       " 'Web Mining/Information Fusion': [<networkx.classes.graph.Graph at 0x1d245440e10>,\n",
       "  <networkx.classes.graph.Graph at 0x1d2454419e8>,\n",
       "  <networkx.classes.graph.Graph at 0x1d2454b0e10>],\n",
       " 'Semantic Web/Description Logics': [<networkx.classes.graph.Graph at 0x1d2454b6240>,\n",
       "  <networkx.classes.graph.Graph at 0x1d2455f2898>],\n",
       " 'Data Mining/Association Rules': [<networkx.classes.graph.Graph at 0x1d2455f8cc0>,\n",
       "  <networkx.classes.graph.Graph at 0x1d2457609e8>],\n",
       " 'Database Systems/XML Data': [<networkx.classes.graph.Graph at 0x1d24586ab00>],\n",
       " 'Information Retrieval': [<networkx.classes.graph.Graph at 0x1d245b6c2e8>,\n",
       "  <networkx.classes.graph.Graph at 0x1d245cb12b0>]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "DATASET = \"./data\"\n",
    "\n",
    "def loadData(directoryPath):\n",
    "    \"\"\"Load the data of all files of the folder given in parameter.\"\"\"\n",
    "    files = os.listdir(directoryPath)\n",
    "    # dictionary wrt to the following format: {topic1:[[network1],[network2]]}\n",
    "    allGraphsOfEachTopic = {}\n",
    "    for file in files:\n",
    "        topic = \"\"\n",
    "        if \"T16\" in file:\n",
    "            topic = \"Data Mining/Association Rules\"\n",
    "        elif \"T107\" in file:\n",
    "            topic = \"Web Services\"\n",
    "        elif \"T131\" in file:\n",
    "            topic = \"Bayesian Networks/Belief function\"\n",
    "        elif \"T144\" in file:\n",
    "            topic = \"Web Mining/Information Fusion\"\n",
    "        elif \"T145\" in file:\n",
    "            topic = \"Semantic Web/Description Logics\"\n",
    "        elif \"T162\" in file:\n",
    "            topic = \"Machine Learning\"\n",
    "        elif \"T24\" in file:\n",
    "            topic = \"Database Systems/XML Data\"\n",
    "        elif \"T75\" in file:\n",
    "            topic = \"Information Retrieval\"\n",
    "        else:\n",
    "            topic = \"Unknown\"\n",
    "\n",
    "        graphToBuild = nx.Graph()\n",
    "        graphToBuild.name = topic.replace('/', ' ').replace(' ', '')\n",
    "\n",
    "        # constant\n",
    "        VERTEX = 0\n",
    "        EDGE = 1\n",
    "        TRIANGLE = 2\n",
    "\n",
    "        f = open(DATASET+\"/\"+file)\n",
    "\n",
    "        # Vertices: Int \"String\" Int -> NodeID, personName, #papers\n",
    "        # Edges: Int Int Int -> sourceNodeID, DestNodeID, #coauthoredPapers\n",
    "        # Triangles: Int,Int,Int,Int -> NodeID1, NodeID2, NodeID3, #coauthoredPapers\n",
    "        for line in f:\n",
    "            if \"*Vertices\" in line:\n",
    "                typeOfLine = VERTEX\n",
    "            elif \"*Edges\" in line:\n",
    "                typeOfLine = EDGE\n",
    "            elif \"*Triangles\" in line:\n",
    "                typeOfLine = TRIANGLE\n",
    "            else:\n",
    "                if typeOfLine == VERTEX:\n",
    "                    graph_edge_list = [s.replace(' ','') for s in re.split('\"', line)]\n",
    "                    graphToBuild.add_node(graph_edge_list[0], name=graph_edge_list[1], nbpapers=graph_edge_list[2])\n",
    "                elif typeOfLine == EDGE:\n",
    "                    graph_edge_list = line.split()\n",
    "                    graphToBuild.add_edge(graph_edge_list[0], graph_edge_list[1], coauthoredPapers=graph_edge_list[2])\n",
    "                elif typeOfLine == TRIANGLE:\n",
    "                    graph_edge_list = line.split(',')\n",
    "                    graphToBuild.add_edge(graph_edge_list[0], graph_edge_list[1], coauthoredPapersTriangle=graph_edge_list[3])\n",
    "                    graphToBuild.add_edge(graph_edge_list[0], graph_edge_list[2], coauthoredPapersTriangle=graph_edge_list[3])\n",
    "                    graphToBuild.add_edge(graph_edge_list[1], graph_edge_list[2], coauthoredPapersTriangle=graph_edge_list[3])\n",
    "\n",
    "        if topic in allGraphsOfEachTopic:\n",
    "            allGraphsOfEachTopic[topic].append(graphToBuild)\n",
    "        else:\n",
    "            allGraphsOfEachTopic[topic] = [graphToBuild]\n",
    "\n",
    "    return allGraphsOfEachTopic\n",
    "\n",
    "allData = loadData(DATASET)\n",
    "print(\"> All data loaded\")\n",
    "DictTable(allData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "## Implementation of the betweenness centrality measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgesBetweenessCentrality(graph, edge):\n",
    "    \"\"\"computes the betweenness centrality of a given edge\"\"\"\n",
    "    centrality = 0\n",
    "    nodes = list(graph.nodes())\n",
    "\n",
    "    for vi in range(len(nodes) - 1):\n",
    "        for vj in range(vi + 1, len(nodes)):\n",
    "            try:\n",
    "                shortestPaths = list(nx.all_shortest_paths(graph, source=nodes[vi], target=nodes[vj]))\n",
    "                nbPathsIncludingEdge = sum(edgeIsInPath([edge[0], edge[1]], path) for path in shortestPaths)\n",
    "                centrality += nbPathsIncludingEdge / len(shortestPaths)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "    return centrality\n",
    "\n",
    "def edgeIsInPath(edge, path):\n",
    "    \"\"\"checks if an edge is contained in a path of an undirected graph (the path is a list of nodes)\"\"\"\n",
    "    pathReversed = list(reversed(path))\n",
    "    n = len(edge)\n",
    "\n",
    "    if edge in (path[i:i + n] for i in range(len(path) + 1 - n)) or edge in (pathReversed[j:j + n] for j in range(len(pathReversed) + 1 - n)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def nodesBetweennessCentrality(graph, node):\n",
    "    \"\"\"betweenness centrality of a given node: not used in this project but implemented for fun\"\"\"\n",
    "    centrality = 0\n",
    "    nodes = list(graph.nodes())\n",
    "\n",
    "    for vi in range(len(nodes)-1):\n",
    "        for vj in range(vi+1, len(nodes)):\n",
    "            if(nodes[vi] == node or nodes[vj] == node):\n",
    "                continue\n",
    "\n",
    "            shortestPaths = list(nx.all_shortest_paths(graph, source=nodes[vi], target=nodes[vj]))\n",
    "            nbPathsIncludingNode = sum(path.count(node) for path in shortestPaths)\n",
    "            centrality += nbPathsIncludingNode / len(shortestPaths)\n",
    "    return centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of visualizations used in Girvan-Newman clustering (see next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(G):\n",
    "    \"\"\"draws the given graph and displays the labels of each edge\"\"\"\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True)\n",
    "    nx.draw_networkx_edge_labels(G, pos)\n",
    "    plt.show()\n",
    "\n",
    "def drawGraphs(G, cnt):\n",
    "    \"\"\"Draw all different subgraphs on the same picture\"\"\"\n",
    "    test = plt.subplot(4, 3, cnt)\n",
    "    if (cnt == 1):\n",
    "        test.title.set_text('Initial state of the graph')\n",
    "    else:\n",
    "        test.title.set_text('State of the graph at iteration: {}, number of communities: {}'.format(cnt-1, nx.number_connected_components(G)))\n",
    "    test.set_yticklabels([])\n",
    "    test.set_xticklabels([])\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx(G, pos)\n",
    "    nx.draw_networkx_edge_labels(G, pos)\n",
    "\n",
    "def drawColoredGraph(G):\n",
    "    \"\"\"# draws a graph where communities are drawn using different colors\"\"\"\n",
    "    test = plt.subplot(4,3,12)\n",
    "    test.set_yticklabels([])\n",
    "    test.set_xticklabels([])\n",
    "    test.title.set_text('Resulting communities ({})'.format(nx.number_connected_components(G)))\n",
    "    pos = nx.spring_layout(G)\n",
    "    colors = ['red', 'green', 'orange', 'cyan', 'magenta', 'yellow', 'pink', 'white', 'brown', 'wheat']\n",
    "    connected_components = nx.connected_component_subgraphs(G)\n",
    "    for index, sg in enumerate(connected_components):\n",
    "        nx.draw_networkx(sg, pos = pos, edge_color = colors[index], node_color = colors[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Girvan-Newman clustering till 10th iteration level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jojo\\Anaconda\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:611: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n",
      "  if cb.is_numlike(alpha):\n"
     ]
    }
   ],
   "source": [
    "def girvanNewmanClustering(graph, nbIteration):\n",
    "    \"\"\"Implementation of the Girvan-Newman clustering algorithm\"\"\"\n",
    "    edges = list(graph.edges)\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    drawGraphs(graph, 1)\n",
    "    cnt = 2\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        return \"Empty graph\"\n",
    "\n",
    "    while(len(list(graph.edges)) > 0 and nbIteration > 0):\n",
    "        nbIteration = nbIteration - 1\n",
    "        highestEdge = \"\"\n",
    "        highestScore = -float('inf')\n",
    "        for edge in edges:\n",
    "            score = edgesBetweenessCentrality(graph, edge)\n",
    "            if score > highestScore:\n",
    "                highestScore = score\n",
    "                highestEdge = edge\n",
    "\n",
    "        graph.remove_edge(highestEdge[0], highestEdge[1])\n",
    "        drawGraphs(graph, cnt)\n",
    "        cnt += 1\n",
    "\n",
    "    drawColoredGraph(graph)\n",
    "\n",
    "    plt.savefig(\"./Figures/GirvanNewman/{}.png\".format(graph.name))\n",
    "    \n",
    "    \n",
    "graph = allData['Semantic Web/Description Logics'][1]\n",
    "girvanNewmanClustering(graph, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Pagerank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRankCentrality(graph, alpha, beta):\n",
    "    # Transposition of matrix\n",
    "    adjacencyMatrix = nx.to_numpy_matrix(graph, weight='None')\n",
    "    amTransposed = np.transpose(adjacencyMatrix)\n",
    "\n",
    "    # Diagonal Matrix\n",
    "    diagonalMatrix = np.zeros([adjacencyMatrix.shape[0], adjacencyMatrix.shape[1]])\n",
    "    row, col = np.diag_indices(diagonalMatrix.shape[0])\n",
    "    # Compute the values that have to be filled into the diagonal\n",
    "    diagonalMatrix[row, col] = [1 / degree[1] for degree in list(graph.degree())]\n",
    "\n",
    "    # Identity matrix\n",
    "    identityMatrix = np.identity(adjacencyMatrix.shape[0])\n",
    "\n",
    "    # Vector of ones\n",
    "    ones = np.ones((adjacencyMatrix.shape[0], 1))\n",
    "    pageRankCentrality = np.dot(beta * np.linalg.inv((identityMatrix - np.dot(alpha * amTransposed, diagonalMatrix))), ones)\n",
    "\n",
    "    return pageRankCentrality\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
